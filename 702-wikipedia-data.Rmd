# Wikipediaデータ {#wikipedia-data}

Twitterのスクレイピングでは10日以上前のツイートを取得できないことを知りました。
また、Googleトレンドは同時に5つまでしか取得できないのに、相対的な値しか取れません。
そこで、Wikipediaの閲覧数を取得する方法を説明します。

```{r}
library(tidyverse)
library(pageviews)
library(WikipediR)
library(gtrendsR)
library(httr)
library(rvest)
```

```{r, echo=FALSE}
library(knitr)
theme_set(theme_bw())
```

## Wikipediaのトレンド {#Trend}

`pageviews`の`article_pageview()`という関数で閲覧数を取得します。
ただし、取得できるデータは2015年7月1日以降に限られます。

試しに、小泉純一郎と小泉進次郎の検索数を取得します。

```{r}
wiki_trend <- article_pageviews(project = "ja.wikipedia",
                                article = c("小泉純一郎", "小泉進次郎"),
                                start = "2015070100",
                                end = Sys.Date())
```

時系列のグラフにします。

```{r}
wiki_trend %>% 
  ggplot() + 
  geom_line(aes(x = date, y = views, colour = article))
```

Googleトレンドをも取得してみます。
スパイクはかなり一致しているように見えます。

```{r}
google_trend <- gtrends(keyword = c("小泉純一郎", "小泉進次郎"), 
                        geo = "JP", 
                        time = str_c("2015-07-01", Sys.Date(), sep = " "))
plot(google_trend)
```

両者が相関しているのかどうかを確認します。

```{r}
full_join(wiki_trend %>% 
            select(date, term = article, wiki = views),
          google_trend$interest_over_time %>% 
            select(date, term = keyword, google = hits) %>% 
            mutate(google = parse_number(google)),
          by = c("date", "term")) %>% 
  ggplot() + 
  geom_point(aes(x = wiki, y = google, colour = term)) + 
  facet_wrap(~term)
```

見にくいので両軸の対数を取ると正の相関がありそうなのが分かります。

```{r}
full_join(wiki_trend %>% 
            select(date, term = article, wiki = views),
          google_trend$interest_over_time %>% 
            select(date, term = keyword, google = hits) %>% 
            mutate(google = parse_number(google)),
          by = c("date", "term")) %>% 
  ggplot() + 
  geom_point(aes(x = wiki, y = google, colour = term)) + 
  scale_x_log10() + 
  scale_y_log10() + 
  facet_wrap(~term)
```

## Wikipediaの記事の情報

`WikipediR`の`page_info()`で記事の情報を取得できます。
役に立ちそうな情報は記事の長さでしょうか。

```{r}
wiki_info <- page_info("ja", "wikipedia", page = "小泉進次郎")
wiki_info$query$pages$`1875027`$length
```

## Wikipediaの記事

`page_content()`で`.html`を取得できます。
`rvest`の`read_html()`で読み込みます。

```{r}
wiki_page <- page_content("ja", "wikipedia", page_name = "小泉進次郎")
wiki_html <- wiki_page$parse$text[[1]] %>% 
  read_html()
```

とりあえず`p`タグのテキストを抽出します。

```{r}
wiki_text <- wiki_html %>% 
  html_nodes("p") %>% 
  html_text()
head(wiki_text)
```

文字列ベクトルを一つの文章にするときは`str_c()`でオプション`collapse = ""`を指定します。

```{r}
wiki_text <- wiki_text %>% 
  str_c(collapse = "")
wiki_text
```

`str_length()`で文字列の長さを調べることができます。

```{r}
str_length(wiki_text)
```

- ページ情報の記事の長さとはだいぶ違います……

## Wikipediaの記事のリンク

Wikipediaの記事のリンクを求めることもできます。
記事にある他のWikipedia記事へのリンク、外部サイトへのリンク、当該記事にリンクを貼っている他のWikipedia記事のリンクをそれぞれ求めることができます。

```{r}
internal_link <- page_links("ja", "wikipedia", page = "小泉進次郎", limit = 500)
external_link <- page_external_links("ja", "wikipedia", page = "小泉進次郎")
back_link <- page_backlinks("ja", "wikipedia", page = "小泉進次郎")
```

- 内部リンクは最大500個までしか取れないようです。

## Wikipediaの記事の作成日の取得*

直接APIを叩いてWikipediaの記事の作成日を取得してみます。

まず、検索したい単語のベクトルを作成します。

```{r}
key.words <- c("新世紀エヴァンゲリオン", "魔法少女まどか☆マギカ", "けものフレンズ")
```

続いて、作成日を格納する空のベクトルを作成します。

```{r}
wiki.date <- NULL
```

最後に、ループによって各単語の記事の作成日を取得します。

```{r}
for (i in key.words) {
  wiki.date <- c(wiki.date,
                 GET("https://ja.wikipedia.org/w/api.php",
                     query = list(action = "query",
                                  prop = "revisions",
                                  rvlimit = 1,
                                  rvprop = "timestamp",
                                  rvdir = "newer",
                                  titles = i,
                                  format = "json")) %>% 
                   content("text") %>% 
                   str_extract("[0-9]+-[0-9]+-[0-9]+"))
}
```

念のため、データフレームにしておきます。

```{r}
df <- tibble(key.words,
             wiki.date)
df
```